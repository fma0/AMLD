{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4aea3ae",
   "metadata": {},
   "source": [
    "# Part 3: Neural Networks for Neural Data of a single Participant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6bb4d8",
   "metadata": {},
   "source": [
    "First we load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e54403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install mne\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9675842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def download_file(url, outfile=None):\n",
    "    if not outfile:\n",
    "        outfile = url.split('/')[-1]\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(outfile, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40fa740",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file('https://github.com/fma0/AMLD/blob/main/902-P.fif?raw=true', outfile='902-P.fif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b8393e",
   "metadata": {},
   "source": [
    "## Data loading and preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8437b",
   "metadata": {},
   "source": [
    "### Exercise 1: \n",
    "Training the first neural network on patient 902\n",
    "\n",
    "1. Load the .fif file that we used before. \n",
    "2. Extract the data and labels\n",
    "3. As descibed in the lecture we need to normalize the data (**Tip**: Each trial seperately, and with the function normalize, given bellow)\n",
    "4. We should use one-hot encoded labels, we can use the function '[to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)' from tensorflow.keras.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(volume):\n",
    "    # input a single trial, of the shape (Channels) x (Time)\n",
    "    stdev = np.std(volume, dtype=np.float64)\n",
    "    mean = np.mean(volume, dtype=np.float64)\n",
    "    return (volume - np.float32(mean)) / np.float32(stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc0b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba3c4e2d",
   "metadata": {},
   "source": [
    "Convolutional neural networks have been traditionally used on images, with 3 dimensions: (X-axis) x (Y-axis) x (color channels). There we don't have multiple channels, which means we just expand the data in the last dimension such that the data shape then is (Channels) x (Time) x (1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673f5db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.expand_dims(data, axis=-1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752bf94a",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "Create the train and test data sets with the train_test_split as in 'Part 2: Machine Learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ea11a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b3c582f",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965702c2",
   "metadata": {},
   "source": [
    "Now let's define the model, it takes as input : the number of classes, the number of recorded EEG channels and the number of time points per trial.\n",
    "\n",
    "**Disclaimer:** We are using EEGNet, which was developped by [Lawhern et. al. (2017)](https://github.com/vlawhern/arl-eegmodels). This network and the below imported code was writen by Lawhern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a788289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(\"https://raw.githubusercontent.com/vlawhern/arl-eegmodels/master/EEGModels.py\")\n",
    "from EEGModels import EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcfa75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167f3419",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "Define these three variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "n_classes = \n",
    "n_channels = \n",
    "n_timepoints = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGNet(n_classes, n_channels, n_timepoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc5e41",
   "metadata": {},
   "source": [
    "Before training we need to compile the network, there we specify the loss, optimizer, learning rate and any other metric that we would like to keep track of. \n",
    "We currently keep track of the accuracy and the AUC score, you are free to add anything else you find meaningfull. Check out https://www.tensorflow.org/api_docs/python/tf/keras/metrics for a list of possibilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaefcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=1e-3), \n",
    "              metrics=['accuracy', 'AUC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac24bb",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca91d5",
   "metadata": {},
   "source": [
    "Then we train the model for 50 epochs. We set the percentage of trials for the validation set with the variable validation_split (here 20%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a71b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_data, labels_train, validation_split=0.2, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0bbb32",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f75643",
   "metadata": {},
   "source": [
    "Let's evaluate how the learning of the model progressed during the training. The history variable contains the progresssion of the models evaluated on the metrics, that we gave to the model before as well as the loss. The history contains the vlues for the train and validation (called with a prefix of 'val_') metrics. We can see all that it keeps track of with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d46a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b834bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, key):\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(key.capitalize())\n",
    "\n",
    "    plt.plot(history.history[key], color='C0', label = 'Train', linewidth=2)\n",
    "    plt.plot(history.history['val_'+key], color='C3', label='Validation', linewidth=2)\n",
    "\n",
    "    legend = ax.legend(fontsize='medium')\n",
    "    ax.set_title('Model ' + key.capitalize())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77603091",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075751af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514bcfb",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "Now we avaluate the performance:\n",
    "1. We will need the predictions and true labels to only have a single dimensional for the f1-score function. Which are both given bellow as predictions and true labels for the train set. Also calculate them for the test set. \n",
    "2. Then evaluate the f-score for the train set and the test set.\n",
    "\n",
    "Is the performance increased?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af14e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = np.argmax(model.predict(train_data), axis=1)\n",
    "true_labels_train = np.argmax(labels_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b083631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e92567b1",
   "metadata": {},
   "source": [
    "## Saliency maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b61ec3",
   "metadata": {},
   "source": [
    "We already precomputed the Saliency maps, they are saved in the file '902-P-Saliency.fif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e1ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file('https://github.com/fma0/AMLD/blob/main/902-P-Saliency.fif?raw=true', \n",
    "              outfile='902-P-Saliency.fif')\n",
    "data_file_s = '902-P-Saliency'\n",
    "epochs_s = mne.read_epochs(data_file_s + '.fif', verbose='error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c0fbbc",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "Plot the topographic map representation of the Saliency maps for both classes and for the following timepoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ceb611",
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = np.arange(0, 0.51, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef30006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
