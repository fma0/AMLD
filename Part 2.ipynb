{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1919923",
   "metadata": {},
   "source": [
    "# Part 2: Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e47d7440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne.decoding import Vectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Models\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mne.viz\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dee5306",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '1-P-cleaned'\n",
    "epochs = mne.read_epochs(data_file + '.fif', verbose='error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190abaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Number of events</th>\n",
       "        <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Events</th>\n",
       "        <td>FN: 99<br>FP: 104<br>FU: 94<br>NN: 30<br>NP: 27<br>NU: 17<br></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Time range</th>\n",
       "        <td>0.000 – 1.496 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Baseline</th>\n",
       "        <td>off</td>\n",
       "    </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<EpochsFIF |  371 events (all good), 0 - 1.49609 sec, baseline off, ~69.7 MB, data loaded,\n",
       " 'FN': 99\n",
       " 'FP': 104\n",
       " 'FU': 94\n",
       " 'NN': 30\n",
       " 'NP': 27\n",
       " 'NU': 17>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs.set_eeg_reference(['O1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8fb2fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Number of events</th>\n",
       "        <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Events</th>\n",
       "        <td>FN: 99<br>FP: 104<br>FU: 94<br>NN: 30<br>NP: 27<br>NU: 17<br></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Time range</th>\n",
       "        <td>0.000 – 1.496 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Baseline</th>\n",
       "        <td>off</td>\n",
       "    </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<EpochsFIF |  371 events (all good), 0 - 1.49609 sec, baseline off, ~69.7 MB, data loaded,\n",
       " 'FN': 99\n",
       " 'FP': 104\n",
       " 'FU': 94\n",
       " 'NN': 30\n",
       " 'NP': 27\n",
       " 'NU': 17>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs.set_eeg_reference('average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c45867de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs_SN = epochs['Standard', 'Novel'] # Standard vs. Novel\n",
    "epochs_UN = epochs['FU', 'FN'] # Unpleasant vs. Neutral\n",
    "epochs_UP = epochs['FU', 'FP'] # Unpleasant vs. Pleasant\n",
    "epochs_NP = epochs['FN', 'FP'] # Neutral vs. Pleasant\n",
    "\n",
    "# Dataset with unpleasant and neutral events\n",
    "data_UN = epochs_UN.get_data()\n",
    "labels_UN = epochs_UN.events[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d95e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_UN, test_data_UN, labels_train_UN, labels_test_UN = train_test_split(data_UN, labels_UN, \n",
    "                                                                                test_size=0.3, \n",
    "                                                                                random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caaf1737",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_0 = make_pipeline(Vectorizer(), StandardScaler(), svm.SVC(kernel='rbf', C=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1765ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 1th fold is 0.8148148148148148\n",
      "\n",
      "Accuracy of 2th fold is 0.5925925925925926\n",
      "\n",
      "Accuracy of 3th fold is 0.5555555555555556\n",
      "\n",
      "Accuracy of 4th fold is 0.6666666666666666\n",
      "\n",
      "Accuracy of 5th fold is 0.5185185185185185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf_svm_0, train_data_UN, labels_train_UN, cv=5)\n",
    "for i in range(len(scores)):   \n",
    "    print('Accuracy of ' + str(i+1) + 'th fold is ' + str(scores[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d90436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm\n",
    "clf_svm_pip = make_pipeline(Vectorizer(), StandardScaler(), svm.SVC(random_state=42))\n",
    "parameters = {'svc__kernel':['linear', 'rbf', 'sigmoid'], 'svc__C':[0.1, 1, 10]}\n",
    "gs_cv_svm = GridSearchCV(clf_svm_pip, parameters, scoring='accuracy', cv=StratifiedKFold(n_splits=5), \n",
    "                         return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ce832d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'svc__C': 0.1, 'svc__kernel': 'linear'}\n",
      "Best Score: 0.6962962962962964\n"
     ]
    }
   ],
   "source": [
    "gs_cv_svm.fit(train_data_UN, labels_train_UN)\n",
    "print('Best Parameters: {}'.format(gs_cv_svm.best_params_))\n",
    "print('Best Score: {}'.format(gs_cv_svm.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "729367f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Clasification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Standard       0.71      0.86      0.77        28\n",
      "       Novel       0.83      0.67      0.74        30\n",
      "\n",
      "    accuracy                           0.76        58\n",
      "   macro avg       0.77      0.76      0.76        58\n",
      "weighted avg       0.77      0.76      0.76        58\n",
      "\n",
      "Accuracy of SVM model: 0.7586206896551724\n",
      "Precision: 0.7696078431372549, Recall: 0.7619047619047619, f1-score:0.7574671445639187\n"
     ]
    }
   ],
   "source": [
    "#Prediction\n",
    "predictions_svm = gs_cv_svm.predict(test_data_UN)\n",
    "\n",
    "#Evaluate\n",
    "report_svm = classification_report(labels_test_UN, predictions_svm, target_names=['Standard', 'Novel'])\n",
    "print('SVM Clasification Report:\\n {}'.format(report_svm))\n",
    "\n",
    "acc_svm = accuracy_score(labels_test_UN, predictions_svm)\n",
    "print(\"Accuracy of SVM model: {}\".format(acc_svm))\n",
    "\n",
    "precision_svm,recall_svm,fscore_svm,support_svm=precision_recall_fscore_support(labels_test_UN,\n",
    "                                                                                predictions_svm,\n",
    "                                                                                average='macro', \n",
    "                                                                                zero_division=0)\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_svm,recall_svm,fscore_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12363cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'logisticregression__penalty': 'l1'}\n",
      "Best Score: 0.7333333333333332\n",
      "LR Clasification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Unpleasant       0.75      0.86      0.80        28\n",
      "     Neutral       0.85      0.73      0.79        30\n",
      "\n",
      "    accuracy                           0.79        58\n",
      "   macro avg       0.80      0.80      0.79        58\n",
      "weighted avg       0.80      0.79      0.79        58\n",
      "\n",
      "Accuracy of LR model: 0.7931034482758621\n",
      "Precision: 0.7980769230769231, Recall: 0.7952380952380952, f1-score:0.7928571428571427\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "clf_lr_pip = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression(random_state=42, \n",
    "                                                                              solver='liblinear'))\n",
    "parameters = {'logisticregression__penalty':['l1', 'l2']}\n",
    "gs_cv_lr = GridSearchCV(clf_lr_pip, parameters, scoring='accuracy')\n",
    "gs_cv_lr.fit(train_data_UN, labels_train_UN)\n",
    "\n",
    "print('Best Parameters: {}'.format(gs_cv_lr.best_params_))\n",
    "print('Best Score: {}'.format(gs_cv_lr.best_score_))\n",
    "\n",
    "#Predictions\n",
    "predictions_lr = gs_cv_lr.predict(test_data_UN)\n",
    "\n",
    "#Evaluation\n",
    "report_lr = classification_report(labels_test_UN, predictions_lr, target_names=['Unpleasant', 'Neutral'])\n",
    "print('LR Clasification Report:\\n {}'.format(report_lr))\n",
    "\n",
    "acc_lr = accuracy_score(labels_test_UN, predictions_lr)\n",
    "print(\"Accuracy of LR model: {}\".format(acc_lr))\n",
    "\n",
    "precision_lr,recall_lr,fscore_lr,support_lr=precision_recall_fscore_support(labels_test_UN,predictions_lr,\n",
    "                                                                            average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_lr,recall_lr,fscore_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a338e4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Clasification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Unpleasant       0.67      0.64      0.65        28\n",
      "     Neutral       0.68      0.70      0.69        30\n",
      "\n",
      "    accuracy                           0.67        58\n",
      "   macro avg       0.67      0.67      0.67        58\n",
      "weighted avg       0.67      0.67      0.67        58\n",
      "\n",
      "Accuracy of LDA model: 0.6724137931034483\n",
      "Precision: 0.6720430107526881, Recall: 0.6714285714285715, f1-score:0.6715350223546944\n"
     ]
    }
   ],
   "source": [
    "# Linear Discriminant Analysis\n",
    "clf_lda_pip = make_pipeline(Vectorizer(), StandardScaler(), LinearDiscriminantAnalysis(solver='svd'))\n",
    "clf_lda_pip.fit(train_data_UN,labels_train_UN)\n",
    "\n",
    "#Predictions\n",
    "predictions_lda = clf_lda_pip.predict(test_data_UN)\n",
    "\n",
    "#Evaluation\n",
    "report_lda = classification_report(labels_test_UN, predictions_lda, target_names=['Unpleasant', 'Neutral'])\n",
    "print('LDA Clasification Report:\\n {}'.format(report_lda))\n",
    "\n",
    "acc_lda = accuracy_score(labels_test_UN, predictions_lda)\n",
    "print(\"Accuracy of LDA model: {}\".format(acc_lda))\n",
    "\n",
    "precision_lda,recall_lda,fscore_lda,support_lda=precision_recall_fscore_support(labels_test_UN,\n",
    "                                                                                predictions_lda,\n",
    "                                                                                average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_lda,recall_lda,fscore_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbb85e67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z4/zm423rsn5dv79lqw76znz1b80000gp/T/ipykernel_1264/2357988812.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0macc_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_lda\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mf1_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfscore_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscore_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscore_lda\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "accuracies.append([acc_svm, acc_lr, acc_lda])\n",
    "f1_scores.append([fscore_svm, fscore_lr, fscore_lda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies, f1_scores = [], []\n",
    "accuracies.append([acc_svm, acc_lr, acc_lda])\n",
    "f1_scores.append([fscore_svm, fscore_lr, fscore_lda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with unpleasant and pleasant events\n",
    "data_UP = epochs_UP.get_data()\n",
    "labels_UP = epochs_UP.events[:,-1]\n",
    "train_data_UP, test_data_UP, labels_train_UP, labels_test_UP = train_test_split(data_UP, labels_UP, \n",
    "                                                                                test_size=0.3, \n",
    "                                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b00c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "clf_svm_pip = make_pipeline(Vectorizer(), StandardScaler(), svm.SVC(random_state=42))\n",
    "parameters = {'svc__kernel':['linear', 'rbf', 'sigmoid'], 'svc__C':[0.1, 1, 10]}\n",
    "gs_cv_svm = GridSearchCV(clf_svm_pip, parameters, scoring='accuracy', cv=StratifiedKFold(n_splits=5), \n",
    "                         return_train_score=True)\n",
    "gs_cv_svm.fit(train_data_UP, labels_train_UP)\n",
    "\n",
    "print('Best Parameters: {}'.format(gs_cv_svm.best_params_))\n",
    "print('Best Score: {}'.format(gs_cv_svm.best_score_))\n",
    "\n",
    "# Make prediction\n",
    "predictions_svm = gs_cv_svm.predict(test_data_UP)\n",
    "#Evaluation\n",
    "report_svm = classification_report(labels_test_UP, predictions_svm, target_names=['Unpleasant', 'Pleasant'])\n",
    "print('SVM Clasification Report:\\n {}'.format(report_svm))\n",
    "\n",
    "acc_svm = accuracy_score(labels_test_UP, predictions_svm)\n",
    "print(\"Accuracy of SVM model: {}\".format(acc_svm))\n",
    "\n",
    "precision_svm,recall_svm,fscore_svm,support_svm=precision_recall_fscore_support(labels_test_UP,\n",
    "                                                                                predictions_svm,\n",
    "                                                                                average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_svm,recall_svm,fscore_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "clf_lr_pip = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression(random_state=42, \n",
    "                                                                              solver='liblinear'))\n",
    "parameters = {'logisticregression__penalty':['l1', 'l2']}\n",
    "gs_cv_lr = GridSearchCV(clf_lr_pip, parameters, scoring='accuracy')\n",
    "gs_cv_lr.fit(train_data_UP, labels_train_UP)\n",
    "\n",
    "print('Best Parameters: {}'.format(gs_cv_lr.best_params_))\n",
    "print('Best Score: {}'.format(gs_cv_lr.best_score_))\n",
    "\n",
    "# Prediction\n",
    "predictions_lr = gs_cv_lr.predict(test_data_UP)\n",
    "\n",
    "#Evaluation\n",
    "report_lr = classification_report(labels_test_UP, predictions_lr, target_names=['Unpleasant', 'Pleasant'])\n",
    "print('LR Clasification Report:\\n {}'.format(report_lr))\n",
    "\n",
    "acc_lr = accuracy_score(labels_test_UP, predictions_lr)\n",
    "print(\"Accuracy of LR model: {}\".format(acc_lr))\n",
    "\n",
    "precision_lr,recall_lr,fscore_lr,support_lr=precision_recall_fscore_support(labels_test_UP,\n",
    "                                                                            predictions_lr,average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_lr,recall_lr,fscore_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04122eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "clf_lda_pip = make_pipeline(Vectorizer(), StandardScaler(), LinearDiscriminantAnalysis(solver='svd'))\n",
    "clf_lda_pip.fit(train_data_UP,labels_train_UP)\n",
    "\n",
    "#Prediction\n",
    "predictions_lda = clf_lda_pip.predict(test_data_UP)\n",
    "\n",
    "#Evaluation\n",
    "report_lda = classification_report(labels_test_UP, predictions_lda, target_names=['Unpleasant', 'Plesant'])\n",
    "print('LDA Clasification Report:\\n {}'.format(report_lda))\n",
    "\n",
    "acc_lda = accuracy_score(labels_test_UP, predictions_lda)\n",
    "print(\"Accuracy of LDA model: {}\".format(acc_lda))\n",
    "\n",
    "precision_lda,recall_lda,fscore_lda,support_lda=precision_recall_fscore_support(labels_test_UP,\n",
    "                                                                                predictions_lda,\n",
    "                                                                                average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_lda,recall_lda,fscore_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43991404",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies.append([acc_svm, acc_lr, acc_lda])\n",
    "f1_scores.append([fscore_svm, fscore_lr, fscore_lda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b346924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with neutral and pleasant events\n",
    "data_NP = epochs_NP.get_data()\n",
    "labels_NP = epochs_NP.events[:,-1]\n",
    "train_data_NP, test_data_NP, labels_train_NP, labels_test_NP = train_test_split(data_NP, labels_NP, \n",
    "                                                                                test_size=0.3, \n",
    "                                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "clf_svm_pip = make_pipeline(Vectorizer(), StandardScaler(), svm.SVC(random_state=42))\n",
    "parameters = {'svc__kernel':['linear', 'rbf', 'sigmoid'], 'svc__C':[0.1, 1, 10]}\n",
    "gs_cv_svm = GridSearchCV(clf_svm_pip, parameters, scoring='accuracy', cv=StratifiedKFold(n_splits=5), \n",
    "                         return_train_score=True)\n",
    "gs_cv_svm.fit(train_data_NP, labels_train_NP)\n",
    "\n",
    "print('Best Parameters: {}'.format(gs_cv_svm.best_params_))\n",
    "print('Best Score: {}'.format(gs_cv_svm.best_score_))\n",
    "\n",
    "# Prediction\n",
    "predictions_svm = gs_cv_svm.predict(test_data_NP)\n",
    "\n",
    "#Evaluation\n",
    "report_svm = classification_report(labels_test_NP, predictions_svm, target_names=['Neutral', 'Pleasant'])\n",
    "print('SVM Clasification Report:\\n {}'.format(report_svm))\n",
    "\n",
    "acc_svm = accuracy_score(labels_test_NP, predictions_svm)\n",
    "print(\"Accuracy of SVM model: {}\".format(acc_svm))\n",
    "\n",
    "precision_svm,recall_svm,fscore_svm,support_svm=precision_recall_fscore_support(labels_test_NP,\n",
    "                                                                                predictions_svm,\n",
    "                                                                                average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_svm,recall_svm,fscore_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "clf_lr_pip = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression(random_state=42, \n",
    "                                                                              solver='liblinear'))\n",
    "parameters = {'logisticregression__penalty':['l1', 'l2']}\n",
    "gs_cv_lr = GridSearchCV(clf_lr_pip, parameters, scoring='accuracy')\n",
    "gs_cv_lr.fit(train_data_NP, labels_train_NP)\n",
    "\n",
    "print('Best Parameters: {}'.format(gs_cv_lr.best_params_))\n",
    "print('Best Score: {}'.format(gs_cv_lr.best_score_))\n",
    "\n",
    "# Prediction\n",
    "predictions_lr = gs_cv_lr.predict(test_data_NP)\n",
    "\n",
    "#Evaluation\n",
    "report_lr = classification_report(labels_test_NP, predictions_lr, target_names=['Neutral', 'Pleasant'])\n",
    "print('LR Clasification Report:\\n {}'.format(report_lr))\n",
    "\n",
    "acc_lr = accuracy_score(labels_test_NP, predictions_lr)\n",
    "print(\"Accuracy of LR model: {}\".format(acc_lr))\n",
    "\n",
    "precision_lr,recall_lr,fscore_lr,support_lr=precision_recall_fscore_support(labels_test_NP,predictions_lr,\n",
    "                                                                            average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_lr,recall_lr,fscore_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01880e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lda_pip = make_pipeline(Vectorizer(), StandardScaler(), LinearDiscriminantAnalysis(solver='svd'))\n",
    "clf_lda_pip.fit(train_data_NP,labels_train_NP)\n",
    "\n",
    "#Prediction\n",
    "predictions_lda = clf_lda_pip.predict(test_data_NP)\n",
    "\n",
    "#Evaluation\n",
    "report_lda = classification_report(labels_test_NP, predictions_lda, target_names=['Neutral', 'Plesant'])\n",
    "print('LDA Clasification Report:\\n {}'.format(report_lda))\n",
    "\n",
    "acc_lda = accuracy_score(labels_test_NP, predictions_lda)\n",
    "print(\"Accuracy of LDA model: {}\".format(acc_lda))\n",
    "\n",
    "precision_lda,recall_lda,fscore_lda,support_lda=precision_recall_fscore_support(labels_test_NP,\n",
    "                                                                                predictions_lda,\n",
    "                                                                                average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_lda,recall_lda,fscore_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb90aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies.append([acc_svm, acc_lr, acc_lda])\n",
    "f1_scores.append([fscore_svm, fscore_lr, fscore_lda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b12880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotEvalMetrics(tasks, labels, evalMetric, metricName):\n",
    "    width = 0.2  # the width of the bars\n",
    "\n",
    "    # Set position of bar on X axis\n",
    "    rects1 = np.arange(len(evalMetric[:][0]))\n",
    "    rects2 = [x + width for x in rects1]\n",
    "    rects3 = [x + width for x in rects2]\n",
    "\n",
    "    plt.bar(rects1, list(zip(*evalMetric))[0], color='#87CEFA', width=width, edgecolor='white', label=labels[0])\n",
    "    plt.bar(rects2, list(zip(*evalMetric))[1], color='#FFE4E1', width=width, edgecolor='white', label=labels[1])\n",
    "    plt.bar(rects3, list(zip(*evalMetric))[2], color='#CD5C5C', width=width, edgecolor='white', label=labels[2])\n",
    "\n",
    "    plt.xlabel('Classification Tasks')\n",
    "    plt.xticks([r + width for r in range(len(evalMetric[:][0]))], tasks)\n",
    "    plt.ylabel(metricName)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3138a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Accuracies\n",
    "tasks = ['UN', 'UP', 'NP']\n",
    "labels = ['SVM', 'LR', 'LDA']\n",
    "plotEvalMetrics(tasks, labels, accuracies, 'Accuracy')\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f663a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot F1 Scores\n",
    "tasks = ['UN', 'UP', 'NP']\n",
    "labels = ['SVM', 'LR', 'LDA']\n",
    "plotEvalMetrics(tasks, labels, f1_scores, 'F1-Scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f1773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee4d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391de54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3896a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd54e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e3ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6711c546",
   "metadata": {},
   "source": [
    "Little projects:\n",
    " - decode identity\n",
    " - bias in the data (reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd67fe6",
   "metadata": {},
   "source": [
    "Get unfiltered data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d69524",
   "metadata": {},
   "source": [
    "Examples on two different dataset -> can choose which one, compare performance, 3 way classification for visual, pleasant vs. unpleasant vs. neutral / familiar vs. novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4604ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
